{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 258 HW1\n",
    "\n",
    "## Pin Tian\n",
    "\n",
    "### PID: A53219987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "\n",
    "def parseData(fname):\n",
    "    for l in urllib.request.urlopen(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "print (\"Reading data...\")\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse255/data/beer/beer_50000.json\"))\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "(1) The table of the number of reviews for each style of beer( in alphabet order) is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altbier                                           \t165\n",
      "American Adjunct Lager                            \t242\n",
      "American Amber / Red Ale                          \t665\n",
      "American Amber / Red Lager                        \t42\n",
      "American Barleywine                               \t825\n",
      "American Black Ale                                \t138\n",
      "American Blonde Ale                               \t357\n",
      "American Brown Ale                                \t314\n",
      "American Dark Wheat Ale                           \t14\n",
      "American Double / Imperial IPA                    \t3886\n",
      "American Double / Imperial Pilsner                \t14\n",
      "American Double / Imperial Stout                  \t5964\n",
      "American IPA                                      \t4113\n",
      "American Malt Liquor                              \t90\n",
      "American Pale Ale (APA)                           \t2288\n",
      "American Pale Lager                               \t123\n",
      "American Pale Wheat Ale                           \t154\n",
      "American Porter                                   \t2230\n",
      "American Stout                                    \t591\n",
      "American Strong Ale                               \t166\n",
      "American Wild Ale                                 \t98\n",
      "Baltic Porter                                     \t514\n",
      "Belgian Dark Ale                                  \t175\n",
      "Belgian IPA                                       \t128\n",
      "Belgian Pale Ale                                  \t144\n",
      "Belgian Strong Dark Ale                           \t146\n",
      "Belgian Strong Pale Ale                           \t632\n",
      "Berliner Weissbier                                \t10\n",
      "BiÃ¨re de Garde                                   \t7\n",
      "Black & Tan                                       \t122\n",
      "Bock                                              \t148\n",
      "Braggot                                           \t26\n",
      "California Common / Steam Beer                    \t11\n",
      "Chile Beer                                        \t11\n",
      "Cream Ale                                         \t69\n",
      "Czech Pilsener                                    \t1501\n",
      "Doppelbock                                        \t873\n",
      "Dortmunder / Export Lager                         \t31\n",
      "Dubbel                                            \t165\n",
      "Dunkelweizen                                      \t61\n",
      "Eisbock                                           \t8\n",
      "English Barleywine                                \t133\n",
      "English Bitter                                    \t267\n",
      "English Brown Ale                                 \t495\n",
      "English Dark Mild Ale                             \t21\n",
      "English India Pale Ale (IPA)                      \t175\n",
      "English Pale Ale                                  \t1324\n",
      "English Pale Mild Ale                             \t21\n",
      "English Porter                                    \t367\n",
      "English Stout                                     \t136\n",
      "English Strong Ale                                \t164\n",
      "Euro Dark Lager                                   \t144\n",
      "Euro Pale Lager                                   \t701\n",
      "Euro Strong Lager                                 \t329\n",
      "Extra Special / Strong Bitter (ESB)               \t667\n",
      "Flanders Oud Bruin                                \t13\n",
      "Flanders Red Ale                                  \t2\n",
      "Foreign / Export Stout                            \t55\n",
      "Fruit / Vegetable Beer                            \t1355\n",
      "German Pilsener                                   \t586\n",
      "Hefeweizen                                        \t618\n",
      "Herbed / Spiced Beer                              \t73\n",
      "Irish Dry Stout                                   \t101\n",
      "Irish Red Ale                                     \t83\n",
      "Keller Bier / Zwickel Bier                        \t23\n",
      "Kristalweizen                                     \t7\n",
      "KÃ¶lsch                                           \t94\n",
      "Lambic - Fruit                                    \t6\n",
      "Lambic - Unblended                                \t10\n",
      "Light Lager                                       \t503\n",
      "Low Alcohol Beer                                  \t7\n",
      "Maibock / Helles Bock                             \t225\n",
      "Milk / Sweet Stout                                \t69\n",
      "Munich Dunkel Lager                               \t141\n",
      "Munich Helles Lager                               \t650\n",
      "MÃ¤rzen / Oktoberfest                             \t557\n",
      "Oatmeal Stout                                     \t102\n",
      "Old Ale                                           \t1052\n",
      "Pumpkin Ale                                       \t560\n",
      "Quadrupel (Quad)                                  \t119\n",
      "Rauchbier                                         \t1938\n",
      "Russian Imperial Stout                            \t2695\n",
      "Rye Beer                                          \t1798\n",
      "Saison / Farmhouse Ale                            \t141\n",
      "Schwarzbier                                       \t53\n",
      "Scotch Ale / Wee Heavy                            \t2776\n",
      "Scottish Ale                                      \t78\n",
      "Scottish Gruit / Ancient Herbed Ale               \t65\n",
      "Smoked Beer                                       \t61\n",
      "Tripel                                            \t257\n",
      "Vienna Lager                                      \t33\n",
      "Weizenbock                                        \t13\n",
      "Wheatwine                                         \t455\n",
      "Winter Warmer                                     \t259\n",
      "Witbier                                           \t162\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "counter = Counter([d['beer/style'] for d in data])\n",
    "styles = []\n",
    "for key in sorted(counter.keys()):\n",
    "    styles.append([key, counter[key]])\n",
    "for style, total in styles:\n",
    "    print ('{:50}\\t{}'.format(style, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) The table of the average value of 'review/taste' for reviews from each style is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altbier                                           \t3.4030303030302997\n",
      "American Adjunct Lager                            \t2.94834710743802\n",
      "American Amber / Red Ale                          \t3.5135338345864495\n",
      "American Amber / Red Lager                        \t3.6904761904761916\n",
      "American Barleywine                               \t4.0642424242424156\n",
      "American Black Ale                                \t3.873188405797102\n",
      "American Blonde Ale                               \t3.2549019607843044\n",
      "American Brown Ale                                \t3.7436305732484128\n",
      "American Dark Wheat Ale                           \t3.6785714285714284\n",
      "American Double / Imperial IPA                    \t4.033324755532863\n",
      "American Double / Imperial Pilsner                \t3.8214285714285707\n",
      "American Double / Imperial Stout                  \t4.479963112005273\n",
      "American IPA                                      \t4.000850960369668\n",
      "American Malt Liquor                              \t2.255555555555554\n",
      "American Pale Ale (APA)                           \t3.649694055944035\n",
      "American Pale Lager                               \t3.215447154471546\n",
      "American Pale Wheat Ale                           \t3.3344155844155834\n",
      "American Porter                                   \t4.0818385650225455\n",
      "American Stout                                    \t3.819796954314729\n",
      "American Strong Ale                               \t3.569277108433727\n",
      "American Wild Ale                                 \t4.188775510204086\n",
      "Baltic Porter                                     \t4.21303501945524\n",
      "Belgian Dark Ale                                  \t3.3400000000000043\n",
      "Belgian IPA                                       \t3.94921875\n",
      "Belgian Pale Ale                                  \t3.7395833333333273\n",
      "Belgian Strong Dark Ale                           \t3.6952054794520586\n",
      "Belgian Strong Pale Ale                           \t4.056170886075938\n",
      "Berliner Weissbier                                \t3.55\n",
      "BiÃ¨re de Garde                                   \t3.928571428571429\n",
      "Black & Tan                                       \t3.9426229508196755\n",
      "Bock                                              \t3.1891891891891926\n",
      "Braggot                                           \t3.8076923076923066\n",
      "California Common / Steam Beer                    \t3.318181818181818\n",
      "Chile Beer                                        \t3.9545454545454546\n",
      "Cream Ale                                         \t3.0289855072463774\n",
      "Czech Pilsener                                    \t3.609593604263799\n",
      "Doppelbock                                        \t3.9828178694157947\n",
      "Dortmunder / Export Lager                         \t3.4193548387096753\n",
      "Dubbel                                            \t3.736363636363636\n",
      "Dunkelweizen                                      \t3.4918032786885242\n",
      "Eisbock                                           \t3.75\n",
      "English Barleywine                                \t4.3609022556390995\n",
      "English Bitter                                    \t3.5374531835205985\n",
      "English Brown Ale                                 \t3.7282828282828318\n",
      "English Dark Mild Ale                             \t3.7857142857142865\n",
      "English India Pale Ale (IPA)                      \t3.471428571428571\n",
      "English Pale Ale                                  \t3.483761329305202\n",
      "English Pale Mild Ale                             \t3.5952380952380953\n",
      "English Porter                                    \t3.7070844686648576\n",
      "English Stout                                     \t3.59926470588235\n",
      "English Strong Ale                                \t3.756097560975609\n",
      "Euro Dark Lager                                   \t3.704861111111105\n",
      "Euro Pale Lager                                   \t2.962910128388023\n",
      "Euro Strong Lager                                 \t2.8480243161094148\n",
      "Extra Special / Strong Bitter (ESB)               \t3.6851574212893596\n",
      "Flanders Oud Bruin                                \t3.9230769230769234\n",
      "Flanders Red Ale                                  \t3.25\n",
      "Foreign / Export Stout                            \t3.2545454545454535\n",
      "Fruit / Vegetable Beer                            \t3.6077490774907868\n",
      "German Pilsener                                   \t3.6672354948805714\n",
      "Hefeweizen                                        \t3.6351132686084493\n",
      "Herbed / Spiced Beer                              \t3.4452054794520572\n",
      "Irish Dry Stout                                   \t3.623762376237626\n",
      "Irish Red Ale                                     \t2.9819277108433724\n",
      "Keller Bier / Zwickel Bier                        \t3.8695652173913047\n",
      "Kristalweizen                                     \t2.7857142857142856\n",
      "KÃ¶lsch                                           \t3.6968085106382964\n",
      "Lambic - Fruit                                    \t3.75\n",
      "Lambic - Unblended                                \t3.3\n",
      "Light Lager                                       \t2.3966202783300226\n",
      "Low Alcohol Beer                                  \t2.714285714285714\n",
      "Maibock / Helles Bock                             \t3.7466666666666733\n",
      "Milk / Sweet Stout                                \t3.7826086956521725\n",
      "Munich Dunkel Lager                               \t3.780141843971635\n",
      "Munich Helles Lager                               \t3.9592307692307447\n",
      "MÃ¤rzen / Oktoberfest                             \t3.5933572710951527\n",
      "Oatmeal Stout                                     \t3.7745098039215637\n",
      "Old Ale                                           \t4.09600760456274\n",
      "Pumpkin Ale                                       \t3.7875000000000054\n",
      "Quadrupel (Quad)                                  \t3.5966386554621788\n",
      "Rauchbier                                         \t4.06785345717226\n",
      "Russian Imperial Stout                            \t4.300371057513937\n",
      "Rye Beer                                          \t4.21357063403782\n",
      "Saison / Farmhouse Ale                            \t3.702127659574472\n",
      "Schwarzbier                                       \t3.622641509433962\n",
      "Scotch Ale / Wee Heavy                            \t4.083393371757802\n",
      "Scottish Ale                                      \t3.7628205128205074\n",
      "Scottish Gruit / Ancient Herbed Ale               \t3.9076923076923054\n",
      "Smoked Beer                                       \t3.1967213114754105\n",
      "Tripel                                            \t3.784046692606993\n",
      "Vienna Lager                                      \t3.530303030303028\n",
      "Weizenbock                                        \t3.384615384615385\n",
      "Wheatwine                                         \t4.186813186813175\n",
      "Winter Warmer                                     \t3.621621621621625\n",
      "Witbier                                           \t3.5277777777777795\n"
     ]
    }
   ],
   "source": [
    "average = defaultdict(int)\n",
    "for d in data:\n",
    "    style, taste = d['beer/style'], d['review/taste']\n",
    "    average[style] += taste / counter[style]\n",
    "for style, taste in sorted(average.items()):\n",
    "    print ('{:50}\\t{}'.format(style, taste))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0 = 3.915205\n",
      "theta_1 = 0.085646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "def feature(datum):\n",
    "    feat = [1]\n",
    "    if datum['beer/style'] == \"American IPA\":\n",
    "        feat.append(1)\n",
    "    else:\n",
    "        feat.append(0)\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "tastes = [d['review/taste'] for d in data]\n",
    "theta,residuals,rank,s = np.linalg.lstsq(X, tastes)\n",
    "print ('theta_0 = %f' % (theta[0]))\n",
    "print ('theta_1 = %f' % (theta[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias such as the average 'review/taste' value for beer that is not American IPA is shown by $\\theta_0$. \n",
    "\n",
    "$\\theta_1$ shows how the 'review/taste' value of 'American IPA' differs from the average of other styles of beer. Because $\\theta_1$ is bigger than 0, it indicates that the 'review/taste' value of American IPA is $0.085646$ higher than the average of other styles of beer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0 = 3.904356\n",
      "theta_1 = 0.056060\n",
      "MSE_train = 0.558107\n",
      "MSE_test = 0.468410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "numrows = len(X)\n",
    "X_train, X_test = X[:numrows // 2], X[numrows // 2:]\n",
    "tastes_train, tastes_test = tastes[:numrows // 2], tastes[numrows // 2:]\n",
    "theta,residuals,rank,s = np.linalg.lstsq(X_train, tastes_train)\n",
    "print ('theta_0 = %f' % (theta[0]))\n",
    "print ('theta_1 = %f' % (theta[1]))\n",
    "\n",
    "tastes_train_predict = np.dot(X_train, theta)\n",
    "tastes_test_predict = np.dot(X_test, theta)\n",
    "\n",
    "MSE_train = np.mean((tastes_train-tastes_train_predict)**2)\n",
    "MSE_test = np.mean((tastes_test-tastes_test_predict)**2)\n",
    "\n",
    "print ('MSE_train = %f' % (MSE_train))\n",
    "print ('MSE_test = %f' % (MSE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =\n",
      " [ 3.60681818 -0.18884943 -0.73802386 -0.11634199  0.45638407  0.19621212\n",
      " -0.45410882  0.26761104  0.33596789  0.8416167   0.35359848 -1.00681818\n",
      "  0.02739474 -0.58598485 -0.17824675  0.3305986   0.20312334 -0.1798951\n",
      "  0.58195733  0.65508658 -0.27061129  0.3449362   0.16385851  0.12651515\n",
      "  0.46842454  0.33580477 -0.74318182 -0.65227273 -0.2937747  -0.41285266\n",
      "  0.13786267  0.25681818  0.76540404 -0.02450111  0.03420746 -0.13106061\n",
      " -0.31931818  0.09815076 -0.10223103  0.12193999  0.11433566 -0.92019846\n",
      " -0.83277972  0.14466111 -0.35681818  0.11320382 -0.62765152 -0.0058248\n",
      " -0.38622995  0.22651515 -0.63806818  0.10049889 -1.23390152 -0.10681818\n",
      "  0.71136364 -0.50681818 -0.20681818 -0.22045455  0.1527972   0.60472028\n",
      "  0.26709486  0.39318182  0.44318182  0.69564175  0.3763751   0.20984848\n",
      "  0.12395105  0.48544372  0.26818182  0.38356643 -0.4664673   0.05895722\n",
      "  0.01818182  0.01699134 -0.09003966]\n",
      "MSE_train = 0.367840\n",
      "MSE_test = 0.433670\n"
     ]
    }
   ],
   "source": [
    "styles_50 = sorted([style for style in counter \n",
    "                       if counter[style] >= 50])\n",
    "index_styles = {style: i + 1 for i, style in enumerate(styles_50)}\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [1] + [0] * len(styles_50)\n",
    "    style = datum['beer/style']\n",
    "    if style in index_styles:\n",
    "        feat[index_styles[style]] = 1\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['review/taste'] for d in data]\n",
    "X_train, X_test = X[:numrows // 2], X[numrows // 2:]\n",
    "y_train, y_test = y[:numrows // 2], y[numrows // 2:]\n",
    "theta,residuals,rank,s = np.linalg.lstsq(X_train, y_train)\n",
    "print('theta =\\n %s' % theta)\n",
    "\n",
    "y_train_predict = np.dot(X_train, theta)\n",
    "y_test_predict = np.dot(X_test, theta)\n",
    "\n",
    "MSE_train = np.mean((y_train-y_train_predict)**2)\n",
    "MSE_test = np.mean((y_test-y_test_predict)**2)\n",
    "\n",
    "print ('MSE_train = %f' % (MSE_train))\n",
    "print ('MSE_test = %f' % (MSE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "## Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "def feature(datum):\n",
    "    return [datum['beer/ABV'], datum['review/taste']]\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [\"American IPA\" in b['beer/style'] for b in data]\n",
    "\n",
    "X_train, X_test = X[:numrows // 2], X[numrows // 2:]\n",
    "y_train, y_test = y[:numrows // 2], y[numrows // 2:]\n",
    "\n",
    "clf = svm.SVC(C=1000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the train data is 92.26%\n",
      "The accuracy on the test data is 85.63%\n"
     ]
    }
   ],
   "source": [
    "train_predict = clf.predict(X_train)\n",
    "test_predict = clf.predict(X_test)\n",
    "accuracy_train = np.mean(train_predict == y_train)\n",
    "accuracy_test = np.mean(test_predict == y_test)\n",
    "print('The accuracy on the train data is %.2f%%' % (accuracy_train * 100))\n",
    "print('The accuracy on the test data is %.2f%%' % (accuracy_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "After observing the features from the text, I chose three more features 'review/appearance', 'review/palate' and the last one, the 'reviews/text' which including 'bitter' and 'amber' or 'orange' because the taste American IPA is bitterness and the appearance of American IPA is amber or orange. \n",
    "\n",
    "We can see that I got a better result of accuracy especially on the test set.\n",
    "\n",
    "In addition, to expedite the training process, a normalization is done by linearly mapping training samples to [0, 1], i.e.\n",
    "$$\\widehat{x}=\\frac{x - \\min(x_{\\text{train}}[n])}{\\max(x_{\\text{train}}[n] - \\min(x_{\\text{train}}[n]))}$$\n",
    "Another reason for doing this normalization is that if it is not done, the SVM classifier won't converge when $C=100000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature(datum):\n",
    "    feat = [datum['review/taste'],\n",
    "            datum['beer/ABV'],\n",
    "            1. if ('bitter' and ('amber' or 'orange')) in datum['review/text'] else 0.,\n",
    "            datum['review/appearance'],\n",
    "            datum['review/palate']]\n",
    "    return feat\n",
    "\n",
    "def normalization(features, training=False):\n",
    "    features = np.asarray(features)\n",
    "    if training:\n",
    "        normalization.mins = np.min(features, axis=0)\n",
    "        normalization.maxs = np.max(features, axis=0)\n",
    "    return (features - normalization.mins) /  (normalization.maxs - normalization.mins)\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [\"American IPA\" in b['beer/style'] for b in data]\n",
    "\n",
    "\n",
    "X_train = normalization(X[:numrows // 2], training=True)\n",
    "X_test = normalization(X[numrows // 2:], training=False)\n",
    "# X_train, X_test = np.asarray(X[:numrows // 2]), np.asarray(X[numrows // 2:])\n",
    "y_train, y_test = y[:numrows // 2], y[numrows // 2:]\n",
    "\n",
    "clf = svm.SVC(C=1000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the train data is 91.36%\n",
      "The accuracy on the test data is 92.19%\n"
     ]
    }
   ],
   "source": [
    "train_predict = clf.predict(X_train)\n",
    "test_predict = clf.predict(X_test)\n",
    "accuracy_train = np.mean(train_predict == y_train)\n",
    "accuracy_test = np.mean(test_predict == y_test)\n",
    "print('The accuracy on the train data is %.2f%%' % (accuracy_train * 100))\n",
    "print('The accuracy on the test data is %.2f%%' % (accuracy_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "The regularization constant C in SVM is generally means that tolerance to the error samples. The larger C, the lower tolerance.\n",
    "\n",
    "But much higher C may result overfitting and cause the compute speed lower. If the C is much smaller, the model won't work well as well. So to get a good performance, it is very important to choose an appropriate value of C.\n",
    "\n",
    "According the results in the problem, increasing c does not effect significantly on our training set results. But it has more effects on my test set results. So the test features I choose may need change to other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1\n",
      "The accuracy on the train data is is 91.36%\n",
      "The accuracy on the test data is 92.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 10\n",
      "The accuracy on the train data is is 91.36%\n",
      "The accuracy on the test data is 92.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 1000\n",
      "The accuracy on the train data is is 91.36%\n",
      "The accuracy on the test data is 92.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 100000\n",
      "The accuracy on the train data is is 91.56%\n",
      "The accuracy on the test data is 91.47%\n"
     ]
    }
   ],
   "source": [
    "for C in [0.1, 10, 1000, 100000]:\n",
    "    clf = svm.SVC(C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_predict = clf.predict(X_train)\n",
    "    test_predict = clf.predict(X_test)    \n",
    "    accuracy_train = np.mean(train_predict == y_train)\n",
    "    accuracy_test = np.mean(test_predict == y_test)\n",
    "    print ('C = %s' % str(C))\n",
    "    print ('The accuracy on the train data is is %.2f%%' % (accuracy_train * 100))\n",
    "    print ('The accuracy on the test data is %.2f%%' % (accuracy_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll = -17328.679513999832\n",
      "ll = -9170.967174702992\n",
      "ll = -8013.542622984981\n",
      "ll = -7757.486442761354\n",
      "ll = -7715.375144080942\n",
      "ll = -7695.684038544232\n",
      "ll = -7661.4969342425575\n",
      "ll = -7632.061939036233\n",
      "ll = -7615.227916256814\n",
      "ll = -7577.9262281454685\n",
      "ll = -7536.181336703033\n",
      "ll = -7452.702321780031\n",
      "ll = -7362.934068746758\n",
      "ll = -7312.702711423136\n",
      "ll = -7298.625222684866\n",
      "ll = -7289.1850920370625\n",
      "ll = -7266.50787459172\n",
      "ll = -7225.066967286063\n",
      "ll = -7168.263686090061\n",
      "ll = -7137.501912725782\n",
      "ll = -7133.470444456122\n",
      "ll = -7131.807304637874\n",
      "ll = -7131.662100045443\n",
      "ll = -7131.616941570908\n",
      "ll = -7131.5461901996105\n",
      "ll = -7131.333102915313\n",
      "ll = -7131.3302725683825\n",
      "ll = -7131.325914682913\n",
      "ll = -7131.3259025171665\n",
      "Final log likelihood = -7131.3259025171665\n",
      "Accuracy = , 92.19%\n",
      "Accuracy = , 91.36%\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from math import exp\n",
    "from math import log\n",
    "from scipy.special import expit\n",
    "\n",
    "def parseData(fname):\n",
    "    for l in urllib.urlopen(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "\n",
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + exp(-x))\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        loglikelihood -= log(1 + exp(-logit))\n",
    "        if not y[i]:\n",
    "            loglikelihood -= logit\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "    print (\"ll =\", loglikelihood)\n",
    "    return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = [0.0]*len(theta)\n",
    "    for i in range(len(X)):\n",
    "    # Fill in code for the derivative\n",
    "        logit = sigmoid(inner(X[i], theta)) \n",
    "        dl -= (logit - y[i]) * X[i]\n",
    "#         dl = list(map(lambda a, b: (a - (logit - y[i]) * b), zip(dl, X[i])))\n",
    "    dl -= 2 * lam * theta\n",
    "    pass\n",
    "  # Negate the return value since we're doing gradient *ascent*\n",
    "    return numpy.array([-x for x in dl])\n",
    "\n",
    "# If we wanted to split with a validation set:\n",
    "#X_valid = X[len(X)/2:3*len(X)/4]\n",
    "#X_test = X[3*len(X)/4:]\n",
    "# Use a library function to run gradient descent (or you can implement yourself!)\n",
    "theta,l,info = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, args = (X_train, y_train, 1.0))\n",
    "print (\"Final log likelihood =\", -l)\n",
    "\n",
    "test_predict = numpy.dot(X_test, theta) > 0    \n",
    "train_predict = numpy.dot(X_train, theta) > 0\n",
    "\n",
    "accuracy_test = np.mean(test_predict == y_test)\n",
    "accuracy_train = np.mean(train_predict == y_train)\n",
    "\n",
    "print ('Accuracy = , %.2f%%' % (accuracy_test * 100))# Compute the accuracy)\n",
    "print ('Accuracy = , %.2f%%' % (accuracy_train * 100))# Compute the accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
