{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 258 HW 2\n",
    "## Pin Tian A53219987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "import scipy.optimize\n",
    "from math import exp\n",
    "from math import log\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parseData(fname):\n",
    "    for l in urlopen(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "print(\"Reading data...\")\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "##################################################\n",
    "# Logistic regression by gradient ascent         #\n",
    "##################################################\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        loglikelihood -= log(1 + exp(-logit))\n",
    "        if not y[i]:\n",
    "            loglikelihood -= logit\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "    # for debugging\n",
    "#     print(\"ll =\" + str(loglikelihood))\n",
    "    return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = [0]*len(theta)\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        for k in range(len(theta)):\n",
    "            dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "            if not y[i]:\n",
    "                dl[k] -= X[i][k]\n",
    "    for k in range(len(theta)):\n",
    "        dl[k] -= lam*2*theta[k]\n",
    "    return np.array([-x for x in dl])\n",
    "\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Train                                          #\n",
    "##################################################\n",
    "\n",
    "def train(lam):\n",
    "    theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, pgtol = 10, \n",
    "                                             args = (X_train, y_train, lam))\n",
    "    return theta\n",
    "\n",
    "##################################################\n",
    "# Predict                                        #\n",
    "##################################################\n",
    "\n",
    "def performance(theta,X,y):\n",
    "    scores = [inner(theta,x) for x in X]\n",
    "    predictions = [s > 0 for s in scores]\n",
    "    correct = [(a==b) for (a,b) in zip(predictions,y)]\n",
    "    acc = sum(correct) * 1.0 / len(correct)\n",
    "    return acc\n",
    "\n",
    "def BER(theta,X,y):\n",
    "    scores = [inner(theta,x) for x in X]\n",
    "    predictions = [s > 0 for s in scores]\n",
    "    tp = sum([(a==b==True) for (a,b) in zip(predictions,y)])\n",
    "    tn = sum([(a==b==False) for (a,b) in zip(predictions,y)])\n",
    "    fp = sum([(a==True and b==False) for (a,b) in zip(predictions,y)])\n",
    "    fn = sum([(a==False and b==True) for (a,b) in zip(predictions,y)])\n",
    "    ber = 1.0 - 0.5*tp/(fn+tp)-0.5*tn/(fp+tn)\n",
    "    return ber,tp,tn,fp,fn\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "We got a very weird results and the reason, I think,is that the imbalance of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [1, datum['review/taste'], datum['review/appearance'], datum['review/aroma'], \n",
    "          datum['review/palate'], datum['review/overall']]\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['beer/ABV'] >= 6.5 for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta:\n",
      "[-4.40507927  0.59370219  0.34404315  0.81246286  0.57522903 -1.06525496]\n",
      "Validation Sets:\tlambda = 1.0\taccuracy = 90.03%\n",
      "Test Sets:\t\tlambda = 1.0\taccuracy = 57.78%\n"
     ]
    }
   ],
   "source": [
    "def feature(datum):\n",
    "    feat = [1, datum['review/taste'], datum['review/appearance'], datum['review/aroma'], \n",
    "          datum['review/palate'], datum['review/overall']]\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['beer/ABV'] >= 6.5 for d in data]\n",
    "\n",
    "length = len(X)\n",
    "X_train = X[:int(length/3)]\n",
    "X_validation = X[int(length/3):int(2*length/3)]\n",
    "X_test = X[int(length*2/3):]\n",
    "y_train = y[:int(length/3)]\n",
    "y_validation = y[int(length/3):int(2*length/3)]\n",
    "y_test = y[int(length*2/3):]\n",
    "\n",
    "##################################################\n",
    "# Validation pipeline                            #\n",
    "##################################################\n",
    "\n",
    "lam = 1.0\n",
    "\n",
    "theta = train(lam)\n",
    "print(\"Theta:\")\n",
    "print(theta)\n",
    "\n",
    "acc_validation = performance(theta,X_validation,y_validation)\n",
    "print ('Validation Sets:\\tlambda = %.1f\\taccuracy = %.2f%%' % \n",
    "       (lam, acc_validation * 100))\n",
    "\n",
    "acc_test = performance(theta,X_test,y_test)\n",
    "print ('Test Sets:\\t\\tlambda = %.1f\\taccuracy = %.2f%%' % (lam, acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Using the stemming method, I create a new feature. We can see that the accuracy of validation sets increases but the accuracy of test sets decreases obviously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import *\n",
    "review_text = [d['review/text'] for d in data]\n",
    "def feature(datum):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = re.sub(r'[^\\w]', ' ', datum.lower())\n",
    "    stemmer.stem(words)\n",
    "    words = words.split()\n",
    "    count = Counter(words)\n",
    "    feat = [1, count['lactic'], count['tart'], count['sour'], \n",
    "            count['citric'], count['sweet'],count['acid'],\n",
    "            count['hop'],count['fruit'],count['salt'],count['spicy']]\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in review_text]\n",
    "y = [d['beer/ABV'] >= 6.5 for d in data]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta:\n",
      "[ 0.05495591  0.0089072   0.05047197 -0.12232022 -0.06518794  0.27678588\n",
      "  0.02316593 -0.00456406  0.40198768 -0.00350025  0.00462623]\n",
      "Validation Sets:\tlambda = 1.0\taccuracy = 94.14%\n",
      "Test Sets:\t\tlambda = 1.0\taccuracy = 36.58%\n"
     ]
    }
   ],
   "source": [
    "X_train = X[:int(length/3)]\n",
    "X_validation = X[int(length/3):int(length*2/3)]\n",
    "X_test = X[int(length*2/3):]\n",
    "y_train = y[:int(length/3)]\n",
    "y_validation = y[int(length/3):int(length*2/3)]\n",
    "y_test = y[int(length*2/3):]\n",
    "\n",
    "##################################################\n",
    "# Validation pipeline                            #\n",
    "##################################################\n",
    "\n",
    "lam = 1.0\n",
    "\n",
    "theta = train(lam)\n",
    "print (\"Theta:\")\n",
    "print (theta)\n",
    "\n",
    "acc_validation = performance(theta,X_validation,y_validation)\n",
    "print ('Validation Sets:\\tlambda = %.1f\\taccuracy = %.2f%%' % \n",
    "       (lam, acc_validation * 100))\n",
    "\n",
    "acc_test = performance(theta,X_test,y_test)\n",
    "print ('Test Sets:\\t\\tlambda = %.1f\\taccuracy = %.2f%%' % \n",
    "       (lam, acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of true positives:\t5806\n",
      "The number of true negatives:\t290\n",
      "The number of false positives:\t10465\n",
      "The number of false negatives:\t106\n",
      "The Balanced Error Rate of the classifier on the test set:\tBER = 49.55%\n"
     ]
    }
   ],
   "source": [
    "ber,tp,tn,fp,fn = BER(theta,X_test,y_test)\n",
    "print (\"The number of true positives:\\t\" + str(tp))\n",
    "print (\"The number of true negatives:\\t\" + str(tn))\n",
    "print (\"The number of false positives:\\t\" + str(fp))\n",
    "print (\"The number of false negatives:\\t\" + str(fn))\n",
    "print ('The Balanced Error Rate of the classifier on \\\n",
    "the test set:\\tBER = %.2f%%' % (ber * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Below is the revised weighted likelihood logistic regression function based on the fomula in piazza provied by TA.\n",
    "\n",
    "We can see that the BER decreases which means the data becomes more balanced than initial one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEGATIVE Log-likelihood\n",
    "def f1(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    num_t = sum(y)\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        #if lam == 1:\n",
    "        #    print (sigmoid(logit))\n",
    "        if y[i]:\n",
    "            loglikelihood += np.log(sigmoid(logit))*len(y)/(2*num_t)\n",
    "        else:\n",
    "            loglikelihood += np.log(1 - sigmoid(logit))*len(y)/(2*(len(y)-num_t))\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "    # for debugging\n",
    "    print(\"ll =\" + str(loglikelihood))\n",
    "    return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime1(theta, X, y, lam):\n",
    "    num_t = sum(y)\n",
    "    dl = [0]*len(theta)\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        for k in range(len(theta)):\n",
    "            if y[i]:\n",
    "                dl[k] += X[i][k] * (1 - sigmoid(logit))*len(y)/(2*num_t)\n",
    "            else:\n",
    "                dl[k] -= X[i][k] * sigmoid(logit)*len(y)/(2*(len(y)-num_t))\n",
    "    for k in range(len(theta)):\n",
    "        dl[k] -= lam*2*theta[k]\n",
    "    return np.array([-x for x in dl])\n",
    "\n",
    "def train1(lam):\n",
    "    theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f1, [0]*len(X[0]), fprime1, \n",
    "                                             pgtol = 10, args = (X_train, y_train, lam))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll =-11551.990911213648\n",
      "ll =-12204.0383706111\n",
      "ll =-11446.1663541375\n",
      "ll =-11394.892982016388\n",
      "ll =-11370.217356722245\n",
      "ll =-11357.996071816313\n",
      "ll =-11351.821113572705\n",
      "ll =-11350.095906291543\n",
      "ll =-11348.926420022355\n",
      "ll =-11348.2079556363\n",
      "Theta:\n",
      "[-0.17202488  0.02642727  0.36315843 -0.19203621 -0.14481634  0.24089245\n",
      "  0.00976002 -0.02653605  0.46849656 -0.03923761 -0.09551067]\n",
      "\n",
      "\n",
      "Train sets\n",
      "The number of true positives:\t4745\n",
      "The number of true negatives:\t4168\n",
      "The number of false positives:\t2121\n",
      "The number of false negatives:\t5632\n",
      "The Balanced Error Rate of the classifier:\tBER =44.00%\n",
      "\n",
      "\n",
      "Validation sets\n",
      "The number of true positives:\t4855\n",
      "The number of true negatives:\t4142\n",
      "The number of false positives:\t2139\n",
      "The number of false negatives:\t5531\n",
      "The Balanced Error Rate of the classifier:\tBER = 43.65%\n",
      "\n",
      "\n",
      "Test sets\n",
      "The number of true positives:\t4749\n",
      "The number of true negatives:\t4135\n",
      "The number of false positives:\t2126\n",
      "The number of false negatives:\t5657\n",
      "The Balanced Error Rate of the classifier:\tBER =44.16%\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "randInd = np.arange(length)\n",
    "np.random.shuffle(randInd)\n",
    "X_res = [X[ind] for ind in randInd]\n",
    "y_res = [y[ind] for ind in randInd]\n",
    "\n",
    "X_train = X_res[:int(length/3)]\n",
    "X_validation = X_res[int(length/3):int(length*2/3)]\n",
    "X_test = X_res[int(length*2/3):]\n",
    "y_train = y_res[:int(length/3)]\n",
    "y_validation = y_res[int(length/3):int(length*2/3)]\n",
    "y_test = y_res[int(length*2/3):]\n",
    "\n",
    "lam = 1.0\n",
    "theta = train1(lam)\n",
    "print (\"Theta:\")\n",
    "print (theta)\n",
    "print ('\\n')\n",
    "\n",
    "print (\"Train sets\")\n",
    "ber,tp,tn,fp,fn = BER(theta,X_train,y_train)\n",
    "print (\"The number of true positives:\\t\" + str(tp))\n",
    "print (\"The number of true negatives:\\t\" + str(tn))\n",
    "print (\"The number of false positives:\\t\" + str(fp))\n",
    "print (\"The number of false negatives:\\t\" + str(fn))\n",
    "print ('The Balanced Error Rate of the classifier:\\tBER =%.2f%%' \n",
    "       % (ber * 100))\n",
    "print ('\\n')\n",
    "\n",
    "print (\"Validation sets\")\n",
    "ber,tp,tn,fp,fn = BER(theta,X_validation,y_validation)\n",
    "print (\"The number of true positives:\\t\" + str(tp))\n",
    "print (\"The number of true negatives:\\t\" + str(tn))\n",
    "print (\"The number of false positives:\\t\" + str(fp))\n",
    "print (\"The number of false negatives:\\t\" + str(fn))\n",
    "print ('The Balanced Error Rate of the classifier:\\tBER = %.2f%%'\n",
    "       % (ber * 100))\n",
    "print ('\\n')\n",
    "\n",
    "print (\"Test sets\")\n",
    "ber,tp,tn,fp,fn = BER(theta,X_test,y_test)\n",
    "print (\"The number of true positives:\\t\" + str(tp))\n",
    "print (\"The number of true negatives:\\t\" + str(tn))\n",
    "print (\"The number of false positives:\\t\" + str(fp))\n",
    "print (\"The number of false negatives:\\t\" + str(fn))\n",
    "print ('The Balanced Error Rate of the classifier:\\tBER =%.2f%%' \n",
    "       % (ber * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "We can see that there is no obviously different among lambda = [0, 0.01, 0.1, 1, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lmabda=0.00:\n",
      "ll =-11551.990911213648\n",
      "ll =-12203.038370611097\n",
      "ll =-11446.105032640478\n",
      "ll =-11394.744218780688\n",
      "ll =-11369.962923716057\n",
      "ll =-11357.662304539635\n",
      "ll =-11351.409160837251\n",
      "ll =-11349.651307371056\n",
      "ll =-11348.45206192654\n",
      "ll =-11347.704938367928\n",
      "theta:\n",
      "[-0.17234443  0.02676684  0.36676928 -0.19405013 -0.1465711   0.24104964\n",
      "  0.00988413 -0.02649154  0.47010122 -0.03975287 -0.09598917]\n",
      "The BER for train sets:\t\tBER = 44.00%\n",
      "The BER for validation sets:\tBER = 43.67%\n",
      "The BER for test sets:\t\tBER = 44.14%\n",
      "\n",
      "\n",
      "Lmabda=0.01:\n",
      "ll =-11551.990911213648\n",
      "ll =-12203.0483706111\n",
      "ll =-11446.10564622349\n",
      "ll =-11394.745707761826\n",
      "ll =-11369.965472119213\n",
      "ll =-11357.665649408862\n",
      "ll =-11351.413293321097\n",
      "ll =-11349.655770016609\n",
      "ll =-11348.456826601841\n",
      "ll =-11347.709995823012\n",
      "theta:\n",
      "[-0.17234123  0.02676342  0.36673279 -0.19402975 -0.14655339  0.24104808\n",
      "  0.00988288 -0.026492    0.47008509 -0.03974767 -0.09598433]\n",
      "The BER for train sets:\t\tBER = 44.00%\n",
      "The BER for validation sets:\tBER = 43.67%\n",
      "The BER for test sets:\t\tBER = 44.14%\n",
      "\n",
      "\n",
      "Lmabda=0.10:\n",
      "ll =-11551.990911213648\n",
      "ll =-12203.138370611096\n",
      "ll =-11446.111168136209\n",
      "ll =-11394.75910736017\n",
      "ll =-11369.988404051655\n",
      "ll =-11357.695746699148\n",
      "ll =-11351.450473873183\n",
      "ll =-11349.695918640144\n",
      "ll =-11348.499689397684\n",
      "ll =-11347.755487918423\n",
      "theta:\n",
      "[-0.17231246  0.02673261  0.36640475 -0.19384653 -0.14639414  0.24103397\n",
      "  0.00987162 -0.02649613  0.46993994 -0.03970094 -0.09594084]\n",
      "The BER for train sets:\t\tBER = 44.00%\n",
      "The BER for validation sets:\tBER = 43.67%\n",
      "The BER for test sets:\t\tBER = 44.14%\n",
      "\n",
      "\n",
      "Lmabda=1.00:\n",
      "ll =-11551.990911213648\n",
      "ll =-12204.0383706111\n",
      "ll =-11446.1663541375\n",
      "ll =-11394.892982016388\n",
      "ll =-11370.217356722245\n",
      "ll =-11357.996071816313\n",
      "ll =-11351.821113572705\n",
      "ll =-11350.095906291543\n",
      "ll =-11348.926420022355\n",
      "ll =-11348.2079556363\n",
      "theta:\n",
      "[-0.17202488  0.02642727  0.36315843 -0.19203621 -0.14481634  0.24089245\n",
      "  0.00976002 -0.02653605  0.46849656 -0.03923761 -0.09551067]\n",
      "The BER for train sets:\t\tBER = 44.00%\n",
      "The BER for validation sets:\tBER = 43.65%\n",
      "The BER for test sets:\t\tBER = 44.16%\n",
      "\n",
      "\n",
      "Lmabda=100.00:\n",
      "ll =-11551.990911213648\n",
      "ll =-12303.038370611097\n",
      "ll =-11451.89115387198\n",
      "ll =-11408.379904832738\n",
      "ll =-11391.918890330744\n",
      "ll =-11385.1988162176\n",
      "ll =-11383.204609984674\n",
      "ll =-11382.848921379762\n",
      "theta:\n",
      "[-0.14489279  0.00724514  0.17377242 -0.09096739 -0.04689564  0.22290511\n",
      "  0.00287928 -0.02557643  0.36981293 -0.01005921 -0.06951783]\n",
      "The BER for train sets:\t\tBER = 43.96%\n",
      "The BER for validation sets:\tBER = 43.65%\n",
      "The BER for test sets:\t\tBER = 44.17%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lam in [0, 0.01, 0.1, 1, 100]:\n",
    "    print('Lmabda=%.2f:' % (lam))\n",
    "    theta = train1(lam)\n",
    "    print(\"theta:\")\n",
    "    print(theta)\n",
    "    ber_train,_,_,_,_ = BER(theta,X_train,y_train)\n",
    "    ber_validation,_,_,_,_ = BER(theta,X_validation,y_validation)\n",
    "    ber_test,_,_,_,_ = BER(theta,X_test,y_test)\n",
    "    print ('The BER for train sets:\\t\\tBER = %.2f%%' % \n",
    "           (ber_train * 100))\n",
    "    print ('The BER for validation sets:\\tBER = %.2f%%' % \n",
    "           (ber_validation * 100))\n",
    "    print ('The BER for test sets:\\t\\tBER = %.2f%%' % \n",
    "           (ber_test * 100))\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.91553267e-04  3.36316078e-03 -4.92016461e-03  1.22605289e-02\n",
      "   8.02994923e-01  2.01250020e-04  5.90516350e-01  7.22369875e-02\n",
      "   1.75711994e-04  3.29456581e-02]\n",
      " [-2.36616086e-03 -8.55889699e-03 -1.91717318e-02  1.45794087e-02\n",
      "  -5.92410182e-01  4.74330032e-04  8.05130166e-01 -4.97366274e-03\n",
      "  -1.28579556e-03  1.14021305e-02]\n",
      " [ 3.99780701e-03  4.58510040e-02  1.01611015e-01  1.87044464e-03\n",
      "  -6.24232301e-02 -5.23321160e-05 -3.73196976e-02  9.90698551e-01\n",
      "   9.18564879e-04  2.79198868e-02]\n",
      " [-1.52012269e-04  2.00218881e-02 -1.04163870e-02  2.44992682e-02\n",
      "  -1.80912429e-02  8.60712007e-05 -2.81132009e-02 -3.02427712e-02\n",
      "   2.45592725e-03  9.98424798e-01]\n",
      " [ 2.55382520e-02  2.24799457e-01  9.67585118e-01  7.19960887e-03\n",
      "  -2.31187652e-03  9.62965425e-03  2.29568098e-02 -1.09118707e-01\n",
      "   9.38463609e-04  2.70997623e-03]\n",
      " [ 3.54769091e-02  9.72027558e-01 -2.29341823e-01  1.34748385e-02\n",
      "  -4.20493294e-03  8.77575267e-03  1.83813401e-03 -2.11898082e-02\n",
      "   7.29985353e-03 -2.28954543e-02]\n",
      " [ 5.66835271e-03 -1.55377546e-02 -3.62210660e-03  9.99363327e-01\n",
      "  -5.81954419e-04  5.54584002e-03 -1.84164136e-02 -8.50413772e-04\n",
      "   2.59682946e-04 -2.48036199e-02]\n",
      " [ 9.96955075e-01 -4.09453361e-02 -1.74561630e-02 -6.65912506e-03\n",
      "  -1.16153723e-03  6.34885323e-02  1.28440550e-03 -3.69526311e-04\n",
      "  -4.48757759e-03  9.63660068e-04]\n",
      " [-6.40484881e-02 -7.96385717e-03 -6.17769008e-03 -5.32627798e-03\n",
      "   2.60276502e-04  9.97852694e-01 -7.26121605e-04  1.31450704e-03\n",
      "  -7.44873354e-03  1.72584485e-04]\n",
      " [ 3.70628217e-03 -7.64907081e-03  5.51204371e-04 -4.79523814e-04\n",
      "  -7.71354670e-04  7.64404986e-03  1.00504146e-03 -5.89435260e-04\n",
      "   9.99930789e-01 -2.29235033e-03]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "y_IPA = [d['beer/style'] == \"American IPA\" for d in data]\n",
    "X_train = X[:int(length/3)]\n",
    "y_train = y_IPA[:int(length/3)]\n",
    "X_train = np.array(X_train)\n",
    "X_train = X_train[:, 1:]\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train)\n",
    "print (pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reconstruction error is:\t 50.08%\n"
     ]
    }
   ],
   "source": [
    "X_PCA = np.dot(X_train, pca.components_.T)\n",
    "error = sum(sum((X_PCA[:,2:] - np.mean(X_PCA[:,2:],0)) ** 2)) / len(X_PCA)\n",
    "print (\"The reconstruction error is:\\t %.2f%%\" % (error*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "\n",
    "I used the X_train set of the initial unshuffled date to plot this scattarplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCA_2D = np.dot(X_train, pca.components_[:2].T)\n",
    "X_IPA = []\n",
    "X_NOIPA = []\n",
    "for a in range(len(y_train)):\n",
    "    if y_train[a] == True:\n",
    "        X_IPA.append(X_PCA_2D[a])\n",
    "    else:\n",
    "        X_NOIPA.append(X_PCA_2D[a])\n",
    "X_IPA = np.array(X_IPA)\n",
    "X_NOIPA = np.array(X_NOIPA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "red = (1,0,0)\n",
    "blue = (0,0,1)\n",
    "plt.scatter(X_NOIPA[:,0],X_NOIPA[:,1], c=blue, alpha=0.5)\n",
    "plt.scatter(X_IPA[:,0],X_IPA[:,1], c=red, alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Others','American IPA'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
